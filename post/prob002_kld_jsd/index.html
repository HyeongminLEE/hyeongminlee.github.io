<!DOCTYPE html>
<html lang="en-us">
<head>


<script async src="https://www.googletagmanager.com/gtag/js?id=UA-108616338-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-108616338-1');
</script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.29" />
  <meta name="author" content="Hyeongmin Lee">
  <meta name="description" content="Integrated M.S/Ph.D Student of Computer Vision">

  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="/css/highlight.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  


  

  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  <link rel="alternate" href="" type="application/rss+xml" title="Hyeongmin Lee&#39;s Website">
  <link rel="feed" href="" type="application/rss+xml" title="Hyeongmin Lee&#39;s Website">

  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/apple-touch-icon.png">

  <link rel="canonical" href="https://hyeongminlee.github.io/post/prob002_kld_jsd/">

  

  <title>Kullback-Leibler Divergence &amp; Jensen-Shannon Divergence | Hyeongmin Lee&#39;s Website</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Hyeongmin Lee&#39;s Website</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Recent Posts</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#blog">
            
            <span>Blog</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
          </a>
        </li>

        
        

        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name" id="top-of-page">Kullback-Leibler Divergence &amp; Jensen-Shannon Divergence</h1>
    

<div class="article-metadata">

  <span class="article-date">
    
        Last updated on
    
    <time datetime="2018-03-03 00:00:00 &#43;0900 KST" itemprop="datePublished">
      Mar 15, 2018
    </time>
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    4 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="https://hyeongminlee.github.io/post/prob002_kld_jsd/#disqus_thread"></a>
  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">

  </ul>
</div>


  

</div>

    <div class="article-style" itemprop="articleBody">
      

<hr />

<p>기계학습에서는 특정 dataset의 분포를 추정하기 위해 <strong>그 집단을 대표하는 Sample을 뽑아 관찰하는 기법</strong>을 사용합니다. 예를 들어 참치와 고등어가 서식하는 바다에서 10마리의 물고기를 잡았을 때 참치 1마리와 9마리의 고등어가 잡혔다면, 우리는 이 바다에 살고 있는 물고기가 대략 참치 10%와 고등어 90%로 이루어져 있다고 짐작할 수 있습니다.</p>

<p>그렇다면 우리가 데이터의 분포를 추정했을 때 <strong>얼마나 잘 추정한 것인지</strong> 측정하는 방법은 없을까요? 오늘 소개해 드릴 <strong>Kullback-Leibler Divergence</strong> 와 <strong>Jensen-Shannon Divergence</strong>는 서로 다른 확률 분포의 차이를 즉정하는 척도입니다. 우리가 추정한 확률 분포와 실제 확률 분포 사이의 차이가 작다면 좋은 추정이라고 할 수 있습니다.</p>

<p>또한 기계학습에서는 복잡한 함수나 분포를 단순화 하여 하나의 간단한 함수로 나타내려는 노력을 많이 합니다. 예를 들어 실제 측정 결과 얻은 복잡한 확률 분포를 비교적 적은 파라미터를 갖는 Gaussian Distribution 등으로 근사한다면 약간의 오차는 있겠지만 정보를 저장하는 데 드는 비용을 크게 절감할 수 있을 것입니다. 이 때도 역시 가장 오차가 적은 Model로 근사하기 위해 위와 같은 척도들을 이용합니다. 이번 포스팅에서는 두 확률 분포 간의 차이를 나타내는 척도인 <strong>Kullback-Leibler Divergence</strong> 와 <strong>Jensen-Shannon Divergence</strong>에 대하여 알아보도록 하겠습니다.
<br/></p>

<ul>
<li><strong>선수 지식:</strong> 이 포스트를 이해하기 위해서는 <a href="/post/prob001_information_theory/">Entropy &amp; Information Theory</a>에 관한 이해가 필요합니다.</li>
<li><strong>이 자료의 도움을 많이 받았습니다:</strong> <a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" target="_blank">COUNT BAYESIE Blog</a></li>
</ul>

<hr />

<div class="adv-coffee">
<div class="hr-light"></div>
<center> &#9749; 대학원생의 커피 한 잔을 위한 광고 &#9749; </center>  
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- Simple AdSense -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1090380267791915"
     data-ad-slot="7104891830"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
<center> &#128578; 봐주셔서 감사합니다 &#128578; </center> 
<div class="hr-light"></div>
</div>

<h2 id="span-style-color-royalblue-font-weight-bold-알파벳-맞추기-게임-span"><span style="color: royalblue; font-weight: bold">알파벳 맞추기 게임</span></h2>

<p><img src="/img/Prob_002/fig1.png" alt="alphabet card" style="width:60%"></p>

<p>앞의 <a href="/post/prob001_information_theory/">Entropy &amp; Information Theory</a> 포스트에서 알파벳 맞추기 게임에 관한 예를 들었었다. 알파벳 맞추기 게임의 규칙에 관한 내용은 이 포스트에서 필요 없으니 선수 지식이 있다면 굳이 읽어 볼 필요는 없다. 우리는 위의 5종류의 알파벳이 쓰인 카드들 중에서 하나를 골라야 하는데, <strong>각 알파벳이 뽑힐 확률</strong>은 다음과 같다고 하자.<br />
<br/></p>

<table>
<thead>
<tr>
<th>카드에 쓰인 알파벳($s_ {i}$)</th>
<th>a</th>
<th>b</th>
<th>c</th>
<th>d</th>
<th>e</th>
</tr>
</thead>

<tbody>
<tr>
<td>그 카드를 뽑을 확률($p_ {i} = P(s_ {i})$)</td>
<td><sup>1</sup>&frasl;<sub>9</sub></td>
<td><sup>2</sup>&frasl;<sub>9</sub></td>
<td><sup>1</sup>&frasl;<sub>3</sub></td>
<td><sup>2</sup>&frasl;<sub>9</sub></td>
<td><sup>1</sup>&frasl;<sub>9</sub></td>
</tr>
</tbody>
</table>

<p><br/></p>

<p>이 확률 분포를 그래프로 나타내면 다음과 같다.
<img src="/img/Prob_002/fig2.png" alt="alphabet card" style="width:60%"></p>

<p>하지만 이와 같이 표로 나타내는 것은 정보를 <strong>저장하는데 많은 용량이 필요</strong>하며 <strong>수학적으로 다루기가 까다롭다</strong>. 그렇기 때문에 우리는 위 확률 분포를 많이 쓰이는 확률 분포 모델로 <strong>근사</strong>하기로 하자.</p>

<p><br/>
<br/>
<br/></p>

<h2 id="span-style-color-royalblue-font-weight-bold-확률-분포-근사-span"><span style="color: royalblue; font-weight: bold">확률 분포 근사</span></h2>

<p>먼저, 복잡하게 생각하기 귀찮으니 <strong>모든 알파벳이 뽑힐 확률을 근사적으로 같다고 생각</strong>해 버리도록 하자.</p>

<p>$$P_ {i} = \frac{1}{5}, i \in \{ 0,1,2,3,4 \}$$</p>

<p>즉, <strong>Uniform Distribution</strong>으로 근사한 것이며, 다음과 같은 확률 분포를 갖게 될 것이다.<br />
<br/></p>

<table>
<thead>
<tr>
<th>카드에 쓰인 알파벳($s_i$)</th>
<th>a</th>
<th>b</th>
<th>c</th>
<th>d</th>
<th>e</th>
</tr>
</thead>

<tbody>
<tr>
<td>그 카드를 뽑을 확률($p_i = P(s_i)$)</td>
<td><sup>1</sup>&frasl;<sub>5</sub></td>
<td><sup>1</sup>&frasl;<sub>5</sub></td>
<td><sup>1</sup>&frasl;<sub>5</sub></td>
<td><sup>1</sup>&frasl;<sub>5</sub></td>
<td><sup>1</sup>&frasl;<sub>5</sub></td>
</tr>
</tbody>
</table>

<p><br/></p>

<p><img src="/img/Prob_002/fig3.png" alt="alphabet card" style="width:60%"></p>

<p>모두 같게 근사를 했더니, 스스로 생각해 봐도 너무 대충 근사한 느낌이 든다. 그래서 두 번째 안으로는 다음과 같이 <strong>Binomial Distribution</strong>을 이용하여 근사해 보도록 하자.
$$P_i = {4 \choose i}(\frac{1}{2})^i(\frac{1}{2})^{4-i}, i \in \{ 0,1,2,3,4 \}$$</p>

<p><br/></p>

<table>
<thead>
<tr>
<th>카드에 쓰인 알파벳($s_i$)</th>
<th>a</th>
<th>b</th>
<th>c</th>
<th>d</th>
<th>e</th>
</tr>
</thead>

<tbody>
<tr>
<td>그 카드를 뽑을 확률($p_i = P(s_i)$)</td>
<td><sup>1</sup>&frasl;<sub>16</sub></td>
<td><sup>1</sup>&frasl;<sub>4</sub></td>
<td><sup>3</sup>&frasl;<sub>8</sub></td>
<td><sup>1</sup>&frasl;<sub>4</sub></td>
<td><sup>1</sup>&frasl;<sub>16</sub></td>
</tr>
</tbody>
</table>

<p><br/>
<img src="/img/Prob_002/fig4.png" alt="alphabet card" style="width:60%"></p>

<p>우리에게는 카드가 나올 확률을 표현할만한 두 가지 모델이 존재한다. 그렇다면 어떤 모델이 실제 확률 분포를 <strong>더 잘 표현</strong>할까? 즉, 어떤 모델이 실제 확률 분포와 <strong>더 닮았을까</strong>? 우리는 해당 모델이 실제 모델과 얼마나 닮았는지 알기 위한 <strong>정량적인 척도</strong>가 필요하다. 이러한 척도중 하나가 바로 <strong>Kullback-Leibler Divergence(KL-Divergence)</strong>이다. 다음 장에서 더 자세히 알아보도록 하자.</p>

<p><br/>
<br/>
<br/></p>

<h2 id="span-style-color-royalblue-font-weight-bold-kullback-leibler-divergence-span"><span style="color: royalblue; font-weight: bold">Kullback-Leibler Divergence</span></h2>

<p><strong>Kullback-Leibler Divergence</strong>는 앞에서 살펴봤듯이 두 모델이 얼마나 비슷하게 생겼는지를 알기 위한 척도인데, 제시한 모델이 실제 모델의 각 item들의 <strong>정보량</strong>을 얼마나 잘 보존하는지를 측정한다. 즉, 원본 데이터가 가지고 있는 <strong>정보량을 잘 보존</strong>할 수록 <strong>원본 데이터와 비슷한 모델</strong>이라는 것이다. 먼저 item $s_i$가 갖는 정보량은 다음과 같이 계산하였다.</p>

<p>$$I_i = -\log(p_i)$$</p>

<p>그리고 원본 확률 분포 $p$ 와 근사된 분포 $q$에 대하여 i번째 item이 가진 정보량의 차이(정보 손실량)는 다음과 같을 것이다.</p>

<p>$$\Delta I_i = \log(p_i) - \log(q_i)$$</p>

<p>$p$에 대하여 이러한 <strong>정보 손실량의 기댓값</strong>을 구한 것이 바로 KL-Divergence이다.</p>

<p>$$D_{KL} (p||q)= E[\log(p_i) - \log(q_i)] = \sum_i {p_i \log \frac{p_i}{q_i}}$$</p>

<p>간단히 말하면, KL-Divergence는 <strong>근사시 발생하는 정보 손실량의 기댓값</strong>이다. 이 값이 작을 수록 더 가깝게 근사한 것이라고 할 수 있다. 그러면 이제 위 두 모델중에 어떤 모델이 원본과 더 가까운지 측정해보도록 하자.</p>

<p><br/></p>

<table>
<thead>
<tr>
<th>Model</th>
<th>KL-Divergence</th>
</tr>
</thead>

<tbody>
<tr>
<td>Uniform Distribution</td>
<td>$\frac{1}{9}\log(\frac{5}{9}) + \frac{2}{9}\log(\frac{10}{9}) + \frac{1}{3}\log(\frac{5}{3}) + \frac{2}{9}\log(\frac{10}{9}) + \frac{1}{9}\log(\frac{5}{9}) = 0.0865$</td>
</tr>

<tr>
<td>Binomial Distribution</td>
<td>$\frac{1}{9}\log(\frac{16}{9}) + \frac{2}{9}\log(\frac{8}{9}) + \frac{1}{3}\log(\frac{8}{9}) + \frac{2}{9}\log(\frac{8}{9}) + \frac{1}{9}\log(\frac{16}{9}) = 0.0362$</td>
</tr>
</tbody>
</table>

<p><br/></p>

<p>계산 결과, <strong>Binomial Distribution</strong>으로 근사하는 것이 더 효과적이라는 결론을 얻을 수 있다.</p>

<p><br/>
<br/>
<br/></p>

<h2 id="span-style-color-royalblue-font-weight-bold-jensen-shannon-divergence-span"><span style="color: royalblue; font-weight: bold">Jensen-Shannon Divergence</span></h2>

<p>당연하게도, KL-Divergence는 근사된 확률 분포가 얼마나 원본과 비슷한지를 측정하는 척도 이외에도, 단순히 <strong>두 대등한 확률 분포가 얼마나 닮았는지</strong>를 측정하는 척도로 쓰일 수 있다. 하지만 포스트를 읽으면서 눈치 채신 분들도 있겠지만, <strong>KL-Divergence는 Symmetric하지 않다</strong>. 즉 $D_ {KL} (p||q) \neq D_ {KL} (q||p)$이다. 그렇기 때문에 KL-Divergence를 Symmetric하게끔 개량한 <strong>Jensen-Shannon Divergence</strong>를 쓴다.</p>

<p>$$\text{JSD}(p,q) = \frac{1}{2}D_ {KL} (p||\frac{p+q}{2}) + \frac{1}{2}D_ {KL} (q||\frac{p+q}{2})$$</p>

<p>그러면 $\text{JSD}(p,q) = \text{JSD}(q,p)$가 되어, 두 확률 분포 사이의 <strong>distance</strong>로서의 역할을 할 수 있게 된다.</p>

    </div>
    
   

<a href="#top-of-page" id="teleport">[top]</a>

</article>

<div class="article-container article-widget">
<div class="hr-light"></div>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1090380267791915"
     data-ad-slot="7104891830"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
</div>

   



<div class="article-container">
  
<div id="disqus_thread"></div>
<script>





(function() { 
var d = document, s = d.createElement('script');
s.src = 'https://hyeongminlee.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017 Hyeongmin Lee &middot; 

      Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic
      theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>

    

    
    
    <script id="dsq-count-scr" src="//hyeongminlee.disqus.com/count.js" async></script>
    

    
    <script async defer src="//maps.googleapis.com/maps/api/js"></script>
    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gmaps.js/0.4.25/gmaps.min.js" integrity="sha256-7vjlAeb8OaTrCXZkCNun9djzuB2owUsaO72kXaFDBJs=" crossorigin="anonymous"></script>
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

