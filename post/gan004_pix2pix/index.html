<!DOCTYPE html>
<html lang="en-us">
<head>


<script async src="https://www.googletagmanager.com/gtag/js?id=UA-108616338-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-108616338-1');
</script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.29" />
  <meta name="author" content="Hyeongmin Lee">
  <meta name="description" content="M.S/Ph.D sutdent of Computer Vision">

  
  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="/css/highlight.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  


  

  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  <link rel="alternate" href="" type="application/rss+xml" title="Hyeongmin Lee&#39;s Website">
  <link rel="feed" href="" type="application/rss+xml" title="Hyeongmin Lee&#39;s Website">

  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/apple-touch-icon.png">

  <link rel="canonical" href="https://hyeongminlee.github.io/post/gan004_pix2pix/">

  

  <title>[GAN]Pix2Pix | Hyeongmin Lee&#39;s Website</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Hyeongmin Lee&#39;s Website</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#publications">
            
            <span>Publications</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Recent Posts</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#blog">
            
            <span>Blog</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
          </a>
        </li>

        
        

        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">[GAN]Pix2Pix</h1>
    

<div class="article-metadata">

  <span class="article-date">
    
        Last updated on
    
    <time datetime="2018-05-01 00:00:00 &#43;0900 KST" itemprop="datePublished">
      May 4, 2018
    </time>
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    4 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="https://hyeongminlee.github.io/post/gan004_pix2pix/#disqus_thread"></a>
  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">

  </ul>
</div>


  

</div>

    <div class="article-style" itemprop="articleBody">
      

<hr />

<p>이전까지의 GAN 알고리즘들을 공부하면서 뭔가 답답함을 느끼신 분들이 계실 수도 있을 것 같습니다. GAN은 <strong>random noise</strong>를 input으로 하기 때문에 <strong>무작위의 데이터</strong>를 생성합니다. 그렇기 때문에 우리가 원하는 데이터를 얻을수도 없고, 어떤 데이터가 나올지 예측하는 일 조차 쉽지 않습니다. 그렇기 때문에 실생활 등에 적용하기에는 한계가 있습니다. 이번 포스트에서 다룰 <strong>Pix2Pix</strong> 알고리즘과 다음 포스트에서 다룰 <strong>DiscoGAN</strong> 알고리즘의 경우 이전의 알고리즘들과는 다르게 실제 Application에 바로 적용이 가능합니다.</p>

<p><strong>Pix2Pix</strong> 알고리즘은 <strong>이미지의 Style을 변형시키는 알고리즘</strong>입니다. <strong>Image Translation</strong>이라고도 불리는데, 이름 그대로 언어로 따지면 번역으로 비유할 수 있습니다. 예를 들면 <strong>Edge 이미지로부터 원본 이미지를 복원</strong>하거나 흑백 이미지에 색을 입히는 <strong>Colorization</strong>도 Pix2Pix를 응용하면 구현이 가능합니다.
<br/></p>

<ul>
<li><strong>선수 지식:</strong> 이 포스트를 이해하기 위해서는 <a href="https://hyeongminlee.github.io/post/gan003_dcgan/" target="_blank">DCGAN</a>에 대한 지식이 필요합니다. GAN에 대한 순차적인 공부를 원하신다면 이 포스트가 포함된 <a href="http://localhost:1313/blog/gan/" target="_blank">시리즈</a>를 순차적으로 보시는 것을 추천드립니다.<br /></li>
<li><strong>참고 논문:</strong> <a href="https://arxiv.org/pdf/1611.07004v1.pdf" target="_blank">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a><br /></li>
<li><strong>소스 코드:</strong> <a href="https://github.com/HyeongminLEE/Tensorflow_Pix2Pix" target="_blank">Tensorflow_Pix2Pix</a><br /></li>
</ul>

<hr />

<p><br/></p>

<h2 id="span-style-color-royalblue-font-weight-bold-introduction-span"><span style="color: royalblue; font-weight: bold">Introduction</span></h2>

<p><strong>Pix2Pix</strong>는 random vector가 아니라 이미지를 input으로 받아서 다른 style의 이미지를 output으로 출력하는 알고리즘이며, 이를 학습시키기 위해서는 input으로 들어갈 dataset과 그 이미지들이 Pix2Pix를 거쳐서 나올 <strong>정답 이미지</strong>가 필요하다. 즉 <strong>Supervised Learning</strong> 알고리즘이다. GAN의 자랑거리 중 하나가 <strong>Unsupervised Learning</strong>라는 점인데, 너무 취약한 단점이 아닌가? 결론부터 말하면 <strong>아니다.</strong> 다음 예시를 보면 쉽게 알 수 있다.</p>

<figure>
  <img src="/img/GAN_004/fig3.jpg" alt="Example of Pix2Pix Application">
  <figcaption style="text-align: center">Pix2Pix의 dataset 예시
</figcaption>
</figure>

<p>그림1은 Pix2Pix를 학습시키기 위한 dataset의 예시이다. 오른쪽은 평범한 운동화의 이미지이고, 왼쪽은 그 운동화의 Edge를 따 놓은 이미지이다. 이미지 처리를 공부해본 분들은 아시겠지만, 오른쪽의 운동화 이미지로부터 왼쪽의 Edge 이미지를 얻는 일은 <strong>굉장히 쉽다.</strong> 주어진 이미지로부터 Edge를 얻는 수많은 알고리즘들이 이미 존재한다. 그렇기 때문에 우리에게 그냥 운동화 이미지만 많이 있다면 <strong>얼마든지 위와 같은 Dataset을 만들 수 있다.</strong> 하지만 왼쪽의 Edge 이미지로부터 오른쪽의 원본으로 되돌아가는 일은 <strong>굉장히 어렵다.</strong> 이 어려운 일을 우리는 <strong>Pix2Pix</strong>에게 왼쪽의 Edge 이미지를 input으로, 오른쪽의 원본 이미지를 output으로 학습시킴으로써 해결할 수 있다는 것이다. <strong>Supervised Learning</strong>이 안좋은 이유는 <strong>dataset을 구축하기가 어렵다는 점</strong>인데, 위 특성에 따르면 Supervised Learning이기는 해도 <strong>dataset 구축이 어렵지 않다.</strong></p>

<p><br/>
<br/>
<br/></p>

<h2 id="span-style-color-royalblue-font-weight-bold-network-structure-span"><span style="color: royalblue; font-weight: bold">Network Structure</span></h2>

<figure>
  <img src="/img/GAN_004/fig1.png" alt="Generator of Pix2Pix">
  <figcaption style="text-align: center">Pix2Pix의 Generator(출처: <a href="https://github.com/taeoh-kim/Pytorch_Pix2Pix">Taeoh Kim's github</a>)
</figcaption>
</figure>

<figure>
  <img src="/img/GAN_004/fig2.png" alt="Discriminator of Pix2Pix">
  <figcaption style="text-align: center">Pix2Pix의 Discriminator(출처: <a href="https://github.com/taeoh-kim/Pytorch_Pix2Pix">Taeoh Kim's github</a>)
</figcaption>
</figure>

<p>그림2와 그림3는 <strong>Pix2Pix</strong> 알고리즘의 <strong>Generator</strong>와 <strong>Discriminator</strong>이다. 얼핏 보기에는 복잡해 보이지만, 뒤에서 부분별로 살펴볼 것이다.</p>

<h3 id="1-generator">1. Generator</h3>

<p>Pix2Pix의 Generator는 전체적인 구조가 일반적인 Generator 구조와는 다르다. 먼저 input과 output이 전부 이미지이기 때문에, 전체적으로 Size가 <strong>줄어들었다가 다시 커지는 구조</strong>를 갖는다. 일반적으로 이러한 네트워크 구조를 <strong>Encoder-Decoder 구조</strong>라고 부르는데 줄어드는 부분이 <strong>Encoder</strong>, 다시 커지는 부분이 <strong>Decoder</strong>에 해당한다. Size가 줄어들 때에는 DCGAN의 Discriminator와 마찬가지로 <strong>Stride가 2인 Convolution</strong>을 이용한다. 다시 커질 때도 역시 DCGAN의 Generator처럼 <strong>Transposed Convolution</strong>을 이용하게 된다. Output Layer의 Activation Function은 <strong>Hyperbolic Tangent</strong>로, -1~1 사이의 값을 갖는다. 그렇기 때문에 input 또한 <strong>-1~1 사이의 값으로 Normalize</strong> 해서 넣어 줘야 한다.</p>

<p>이제, 아까부터 Generator의 구조에서 계속 거슬렸을 저 점선 화살표들에 대해 알아 볼 차례다. 저 화살표들의 정체는 바로 <strong>Skip Connection</strong>이다. Generator의 구조를 보면 Encoder와 Decoder 부분이 완벽한 대칭 구조를 이루고 있기 때문에, <strong>Decoder의 각 output에 대해서 이에 대응하는 Encoder의 output이 존재</strong>한다. 이 사실을 이용하여 우리는 Encoder에서 각각 <strong>Acitvation Function을 거치기 전의 output</strong> 들을 복사하여 이에 대응하는 <strong>Decoder의 Activation Function을 거친 후의 output에 Concatnate한다.</strong> 즉, 그대로 뒤에 붙여주는 것이다. <strong>Encoder-Decoder 구조의 단점</strong>중의 하나가 바로, <strong>중앙의 Feature Dimension이 input보다 작기 때문에 정보의 손실</strong>이 발생한다는 점이다. 그렇기 때문에 Pix2Pix의 output은 input보다 적은 정보만을 가지고 생성해야 하는 어려움이 따른다(input보다 정보가 많으면 많았지 적은 경우는 거의 없을텐데 말이다). 그렇기 때문에 Decoder의 각 Layer들에게 <strong>정보가 손실되기 전 Encoder단의 Feature들을 제공하여 참고</strong>하게 만드는 것이다. 이를 통해 더 선명한 형태의 output을 얻을 수 있다고 한다. 참고로 이렇게 Skip Connection이 추가된 Encoder-Decoder 형태의 네트워크를 <strong>U-Net 구조</strong>라고 한다.</p>

<h3 id="2-discriminator">2. Discriminator</h3>

<p>Discriminator는 DCGAN과 마찬가지로 stride가 2인 Convolution Layer들로 구성되어 있다. 다만 뒤의 두 Layer의 경우 stride가 1인 Valid Convolution(Padding을 안하는 Convolution)을 통해 최종적으로 30by30의 output을 출력하는 모습을 볼 수 있다. 일반적인 Discriminator의 출력이 0~1 사이의 Scalar라는 점을 생각하면 분명 특이한 부분이다. 그 이유는 Discriminate를 이미지의 각 부분별로 진행하기 위함이다. 즉 이미지를 통째로 진짜인지 아닌지를 판별하는 것이 아니라, <strong>이미지의 각 부분이 진짜인지 아닌지를 판별하도록 하는 것</strong>이다. 이를 통해 조금 더 디테일한 부분을 살린 이미지를 얻을 수 있게 된다.</p>

<p><br/>
<br/>
<br/></p>

<h2 id="span-style-color-royalblue-font-weight-bold-training-pix2pix-span"><span style="color: royalblue; font-weight: bold">Training Pix2Pix</span></h2>

<p>지금까지 써 왔던 GAN의 Loss 함수는 다음과 같았다.</p>

<p><br/>
$$Loss_D = -\log(D(x)) -\log(1-D(G(z)))$$
$$Loss_G = -\log(D(G(z)))$$
<br/></p>

<p>하지만 Generator에게는 Discriminator를 속이는 일 말고도 하나의 과제가 더 주어지게 된다. 바로 Output으로 생성한 이미지가 미리 준비된 정답 이미지와 같아야 한다. 즉 예측과 정답 사이의 distance에 해당하는 Loss가 추가되어야 한다. 이 경우 Pix2Pix에서는 다음과 같이 $L_1$ Loss를 이용한다.</p>

<p><br/>
$$L_1(A,B) = \sum _ {x, y} \vert {A(x,y) - B(x,y)} \vert$$
<br/></p>

<p>그러므로 생성된 이미지 $G$와 정답 이미지 $Y$에 대하여 Generator의 Loss는 다음과 같이 수정된다.</p>

<p><br/>
$$Loss_G = -\log(D(G(z))) + L_1(G,Y)$$
<br/></p>

<p><br/>
<br/>
<br/></p>

<h2 id="span-style-color-royalblue-font-weight-bold-pix2pix의-결과-span"><span style="color: royalblue; font-weight: bold">Pix2Pix의 결과</span></h2>

<p>다음은 다양한 데이터들을 이용하여 Pix2Pix 학습을 진행한 결과이다.<br />
왼쪽부터 각각 <strong>input / output / groundtruth</strong>이다.</p>

<figure>
  <img src="/img/GAN_004/fig4_1.png" alt="result of Pix2Pix">
  <img src="/img/GAN_004/fig4_2.png" alt="result of Pix2Pix">
  <figcaption style="text-align: center">Edge2Shoe의 학습 결과.
</figcaption>
</figure>

<figure>
  <img src="/img/GAN_004/fig5_1.png" alt="result of Pix2Pix">
  <img src="/img/GAN_004/fig5_2.png" alt="result of Pix2Pix">
  <img src="/img/GAN_004/fig5_3.png" alt="result of Pix2Pix">
  <img src="/img/GAN_004/fig5_4.png" alt="result of Pix2Pix">
  <img src="/img/GAN_004/fig5_5.png" alt="result of Pix2Pix">
  <img src="/img/GAN_004/fig5_6.png" alt="result of Pix2Pix">
  <img src="/img/GAN_004/fig5_7.png" alt="result of Pix2Pix">
  <img src="/img/GAN_004/fig5_8.png" alt="result of Pix2Pix">
  <figcaption style="text-align: center">Maps2Aerials의 학습 결과.
</figcaption>
</figure>

<figure>
  <img src="/img/GAN_004/fig6_1.png" alt="result of Pix2Pix">
  <img src="/img/GAN_004/fig6_2.png" alt="result of Pix2Pix">
  <img src="/img/GAN_004/fig6_3.png" alt="result of Pix2Pix">
  <img src="/img/GAN_004/fig6_4.png" alt="result of Pix2Pix">
  <img src="/img/GAN_004/fig6_5.png" alt="result of Pix2Pix">
  <img src="/img/GAN_004/fig6_6.png" alt="result of Pix2Pix">
  <img src="/img/GAN_004/fig6_7.png" alt="result of Pix2Pix">
  <img src="/img/GAN_004/fig6_8.png" alt="result of Pix2Pix">
  <figcaption style="text-align: center">Labels2Photo의 학습 결과.
</figcaption>
</figure>

    </div>
    
   

</article>
   
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-1090380267791915"
     data-ad-slot="7104891830"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

   



<div class="article-container">
  
<div id="disqus_thread"></div>
<script>





(function() { 
var d = document, s = d.createElement('script');
s.src = 'https://hyeongminlee.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017 Hyeongmin Lee &middot; 

      Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic
      theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>

    

    
    
    <script id="dsq-count-scr" src="//hyeongminlee.disqus.com/count.js" async></script>
    

    
    <script async defer src="//maps.googleapis.com/maps/api/js"></script>
    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gmaps.js/0.4.25/gmaps.min.js" integrity="sha256-7vjlAeb8OaTrCXZkCNun9djzuB2owUsaO72kXaFDBJs=" crossorigin="anonymous"></script>
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

